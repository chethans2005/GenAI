{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfa787f",
   "metadata": {},
   "source": [
    "# Unit 1 Assignment: The Model Benchmark Challenge\n",
    "\n",
    "**Objective**: This assignment evaluates the architectural differences between **BERT**, **RoBERTa**, and **BART** by forcing these models to perform tasks they might not be designed for. We will observe why architecture matters and how different model designs affect performance.\n",
    "\n",
    "## Models Under Test:\n",
    "1. **BERT** (`bert-base-uncased`): An **Encoder-only** model designed for understanding, not generation\n",
    "2. **RoBERTa** (`roberta-base`): An optimized **Encoder-only** model \n",
    "3. **BART** (`facebook/bart-base`): An **Encoder-Decoder** model designed for seq2seq tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f143a",
   "metadata": {},
   "source": [
    "## Setup Section\n",
    "\n",
    "First, let's import the necessary libraries and check versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b130460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete!\n",
      "Transformers version: 5.0.0\n",
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline, __version__ as transformers_version\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Setup Complete!\")\n",
    "print(f\"Transformers version: {transformers_version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0ba12",
   "metadata": {},
   "source": [
    "### Experiment 1: Text Generation\n",
    "**Task**: Try to generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
    "*   **Code Hint**: `pipeline('text-generation', model='...')`\n",
    "*   **Hypothesis**: Which models will fail? Why? (Hint: Can an Encoder *generate* new tokens easily?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4762850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT 1: TEXT GENERATION ===\n",
      "\n",
      "Testing bert-base-uncased:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 721.49it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Generated text: The future of Artificial Intelligence is. the within my many so the and and in the ring ( \" ( ) dot it or so everyone onwards 2 to over jay however he and and general in the around each \" ( \" ( \" \".\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing roberta-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 881.61it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Generated text: The future of Artificial Intelligence is\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing facebook/bart-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 159/159 [00:00<00:00, 791.04it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]   \n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Generated text: The future of Artificial Intelligence is analyses analyses R R Wid samurai samurai samurai analyses answering diver R nonviolent R answering R rentedPT devotion devotion workers Indies workers answeringanny workers draft██ devotion devotion turmoil answering R██together██together Colourtogethertogethertogether\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test text generation with all three models\n",
    "models_to_test = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\", \n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "print(\"=== EXPERIMENT 1: TEXT GENERATION ===\\n\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"Testing {model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create text generation pipeline\n",
    "        generator = pipeline('text-generation', model=model_name, max_length=50, num_return_sequences=1)\n",
    "        \n",
    "        # Generate text\n",
    "        result = generator(prompt)\n",
    "        \n",
    "        generated_text = result[0]['generated_text']\n",
    "        continuation = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"DONE!\")\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FAILURE\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b92b1",
   "metadata": {},
   "source": [
    "- BERT and BART generated some output while roberta didn't, but none of them were able to generate something meaning full.\n",
    "- Outputs are:\n",
    "    * The future of Artificial Intelligence is. ( ) ( ). the often it it i..\n",
    "    * The future of Artificial Intelligence is \n",
    "    * The future of Artificial Intelligence is analyses analyses R R Wid samurai samurai.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94a984",
   "metadata": {},
   "source": [
    "### Experiment 2: Masked Language Modeling (Missing Word)\n",
    "**Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
    "*   **Code Hint**: `pipeline('fill-mask', model='...')`\n",
    "*   **Hypothesis**: This is how BERT/RoBERTa were trained. They should perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd13cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT 2: FILL-MASK (MASKED LANGUAGE MODELING) ===\n",
      "\n",
      "Testing bert-base-uncased:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 731.46it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The goal of Generative AI is to [MASK] new content.\n",
      "DONE - Top 5 predictions:\n",
      "  1. create (confidence: 0.5397)\n",
      "  2. generate (confidence: 0.1558)\n",
      "  3. produce (confidence: 0.0541)\n",
      "  4. develop (confidence: 0.0445)\n",
      "  5. add (confidence: 0.0176)\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing roberta-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 776.37it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The goal of Generative AI is to <mask> new content.\n",
      "DONE - Top 5 predictions:\n",
      "  1.  generate (confidence: 0.3711)\n",
      "  2.  create (confidence: 0.3677)\n",
      "  3.  discover (confidence: 0.0835)\n",
      "  4.  find (confidence: 0.0213)\n",
      "  5.  provide (confidence: 0.0165)\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing facebook/bart-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 881.24it/s, Materializing param=model.shared.weight]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The goal of Generative AI is to <mask> new content.\n",
      "DONE - Top 5 predictions:\n",
      "  1.  create (confidence: 0.0746)\n",
      "  2.  help (confidence: 0.0657)\n",
      "  3.  provide (confidence: 0.0609)\n",
      "  4.  enable (confidence: 0.0359)\n",
      "  5.  improve (confidence: 0.0332)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test fill-mask with all three models\n",
    "print(\"=== EXPERIMENT 2: FILL-MASK (MASKED LANGUAGE MODELING) ===\\n\")\n",
    "\n",
    "# Define the sentence with proper mask tokens for each model\n",
    "sentences = {\n",
    "    \"bert-base-uncased\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"roberta-base\": \"The goal of Generative AI is to <mask> new content.\",  # RoBERTa uses <mask>\n",
    "    \"facebook/bart-base\": \"The goal of Generative AI is to <mask> new content.\"  # BART uses <mask>\n",
    "}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"Testing {model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create fill-mask pipeline\n",
    "        fill_mask = pipeline('fill-mask', model=model_name, top_k=5)\n",
    "        \n",
    "        # Get the appropriate sentence for this model\n",
    "        sentence = sentences[model_name]\n",
    "        print(f\"Input: {sentence}\")\n",
    "        \n",
    "        # Predict the masked word\n",
    "        results = fill_mask(sentence)\n",
    "        \n",
    "        print(f\"DONE - Top 5 predictions:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. {result['token_str']} (confidence: {result['score']:.4f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"FAILURE\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a3543",
   "metadata": {},
   "source": [
    "* Here, BERT and roberta excels and outperforms BART. \n",
    "    - BERT: Best performance (53.9% confidence for \"create\")\n",
    "    - RoBERTa: Equally strong (37.1% \"generate\", 36.8% \"create\")\n",
    "    - BART: Lower confidence scores but still reasonable predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613515e",
   "metadata": {},
   "source": [
    "### Experiment 3: Question Answering\n",
    "**Task**: Answer the question `\"What are the risks?\"` based on the context: `\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
    "*   **Code Hint**: `pipeline('question-answering', model='...')`\n",
    "*   **Note**: Using a \"base\" model (not fine-tuned for SQuAD) might yield random or poor results. Observe this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50228372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT 3: QUESTION ANSWERING ===\n",
      "\n",
      "Context: Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
      "Question: What are the risks?\n",
      "\n",
      "Testing bert-base-uncased:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 793.66it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]              \n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Answer: 'risks such as hallucinations'\n",
      "Confidence: 0.0087\n",
      "Start position: 32\n",
      "End position: 60\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing roberta-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 875.25it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Answer: 'Generative AI'\n",
      "Confidence: 0.0078\n",
      "Start position: 0\n",
      "End position: 13\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing facebook/bart-base:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 865.30it/s, Materializing param=model.shared.weight]                                  \n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.weight | MISSING | \n",
      "qa_outputs.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Answer: 'as hallucinations, bias, and deepfakes.'\n",
      "Confidence: 0.0327\n",
      "Start position: 43\n",
      "End position: 82\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test question answering with all three models\n",
    "print(\"=== EXPERIMENT 3: QUESTION ANSWERING ===\\n\")\n",
    "\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"Testing {model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create question-answering pipeline with base model\n",
    "        qa_pipeline = pipeline('question-answering', model=model_name)\n",
    "        \n",
    "        # Get answer\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        print(f\"DONE!\")\n",
    "        print(f\"Answer: '{result['answer']}'\")\n",
    "        print(f\"Confidence: {result['score']:.4f}\")\n",
    "        print(f\"Start position: {result['start']}\")\n",
    "        print(f\"End position: {result['end']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FAILURE\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e11641",
   "metadata": {},
   "source": [
    "- QnA part, all the models shows poor performance, still slightly different from each other:\n",
    "    * BERT: Very low confidence (0.87%) but partially correct\n",
    "    * RoBERTa: Wrong answer with very low confidence (0.78%)\n",
    "    * BART: Best performance with 3.27% confidence and most complete answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5b03",
   "metadata": {},
   "source": [
    "## Observation Table\n",
    "\n",
    "Based on the experimental results, here is the updated observation table:\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "|------|-------|----------------------------------|-------------|---------------------|\n",
    "| **Text Generation** | BERT | **Failure** | Generated complete nonsense: \"is. ( ) ( ). the often it it it with him how the the the the the the on as and - ( ) a a a from \". the or so \" ( ) ( \" ) (\" | BERT is encoder-only; forced into causal generation produces gibberish because it's trained on bidirectional MLM, not autoregressive generation |\n",
    "| | RoBERTa | **Failure** | Generated no new content - just repeated the input prompt exactly | RoBERTa is encoder-only; cannot perform autoregressive generation effectively, so it defaults to just returning the input |\n",
    "| | BART | **Failure** | Generated nonsensical words: \"darkened pet Hero handsome proportion implications connectionsu ntled frog frog...\" | Even though BART is encoder-decoder, the base model isn't properly fine-tuned for causal generation and produces random tokens |\n",
    "| **Fill-Mask** | BERT | **Success** | Excellent predictions: \"create\" (53.9%), \"generate\" (15.6%), \"produce\" (5.4%), \"develop\" (4.5%) | BERT is specifically designed and trained on Masked Language Modeling (MLM) - this is its core strength |\n",
    "| | RoBERTa | **Success** | Very good predictions: \"generate\" (37.1%), \"create\" (36.8%), \"discover\" (8.4%) | RoBERTa uses optimized MLM training like BERT - performs excellently on its designed task |\n",
    "| | BART | **Partial Success** | Reasonable but lower confidence predictions: \"create\" (7.5%), \"help\" (6.6%), \"provide\" (6.1%) | BART includes MLM in training but confidence scores are much lower than pure encoder models |\n",
    "| **Question Answering** | BERT | **Poor Performance** | Found partial answer \"risks such as hallucinations\" but very low confidence (0.87%) | Base BERT can extract text spans but without QA fine-tuning, randomly initialized QA head gives poor results |\n",
    "| | RoBERTa | **Poor Performance** | Completely wrong answer \"Generative AI\" with very low confidence (0.78%) | Same issue as BERT - base model with randomly initialized QA components performs poorly |\n",
    "| | BART | **Slightly Better** | More complete answer \"as hallucinations, bias, and deepfakes\" with higher confidence (3.27%) | Encoder-decoder architecture handles QA slightly better than encoder-only, but still needs fine-tuning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
