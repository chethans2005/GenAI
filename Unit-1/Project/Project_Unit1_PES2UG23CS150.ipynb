{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0175c6a3",
   "metadata": {},
   "source": [
    "# Smart Resume Parser using Hugging Face Transformers\n",
    "## Unit-1 NLP Mini-Project\n",
    "\n",
    "**Chethan S** <br>\n",
    "**PES2UG23CS150** <br>\n",
    "**6th Semester C section**\n",
    "\n",
    "**Objective:** Implement a resume parsing system using Hugging Face pipelines to:\n",
    "- Extract named entities (NER)\n",
    "- Generate text summarization\n",
    "- Perform zero-shot classification for skill domains\n",
    "\n",
    "**Models Used:**\n",
    "- BERT-based NER model\n",
    "- `sshleifer/distilbart-cnn-12-6` for summarization\n",
    "- `facebook/bart-large-mnli` for zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886de7de",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "We'll install the transformers library and import the necessary modules for our resume parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79158ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afe455",
   "metadata": {},
   "source": [
    "## 2. Define Resume Text\n",
    "We are just defining a simple resume for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f3f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume text loaded successfully!\n",
      "Text length: 1881 characters\n",
      "Ready for NLP processing...\n"
     ]
    }
   ],
   "source": [
    "# Sample resume text for processing\n",
    "resume_text = \"\"\"\n",
    "Sarah Johnson\n",
    "Machine Learning Engineer\n",
    "Email: sarah.johnson@email.com | Phone: (555) 123-4567\n",
    "Location: San Francisco, California\n",
    "\n",
    "PROFESSIONAL SUMMARY\n",
    "Experienced Machine Learning Engineer with 5+ years of expertise in developing AI-powered solutions. \n",
    "Skilled in Python programming, deep learning frameworks like TensorFlow and PyTorch, and cloud platforms \n",
    "including AWS and Azure. Proven track record in implementing natural language processing models and \n",
    "computer vision applications for enterprise clients.\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "• Programming Languages: Python, JavaScript, SQL, R\n",
    "• Machine Learning: TensorFlow, PyTorch, Scikit-learn, Keras\n",
    "• Web Development: React, Node.js, Flask, Django\n",
    "• Cloud Platforms: AWS, Azure, Google Cloud Platform\n",
    "• Cybersecurity: Penetration testing, Security auditing, Vulnerability assessment\n",
    "• Artificial Intelligence: Natural Language Processing, Computer Vision, Neural Networks\n",
    "• Databases: PostgreSQL, MongoDB, MySQL\n",
    "\n",
    "WORK EXPERIENCE\n",
    "Senior ML Engineer at TechCorp Inc., San Francisco (2021-Present)\n",
    "- Developed and deployed machine learning models for fraud detection achieving 94% accuracy\n",
    "- Built recommendation systems using collaborative filtering and deep learning techniques\n",
    "- Led a team of 4 data scientists in implementing MLOps pipelines on AWS\n",
    "\n",
    "Data Scientist at DataSolutions LLC, California (2019-2021)\n",
    "- Created predictive analytics models for customer behavior analysis\n",
    "- Implemented web scraping solutions and API integrations using Python\n",
    "- Designed secure data processing pipelines with encryption and access controls\n",
    "\n",
    "EDUCATION\n",
    "Master of Science in Computer Science - Stanford University (2019)\n",
    "Bachelor of Engineering in Software Engineering - University of California, Berkeley (2017)\n",
    "\n",
    "CERTIFICATIONS\n",
    "- AWS Certified Machine Learning Specialty\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Certified Ethical Hacker (CEH)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Resume text loaded successfully!\")\n",
    "print(f\"Text length: {len(resume_text)} characters\")\n",
    "print(\"Ready for NLP processing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b91d3e",
   "metadata": {},
   "source": [
    "## 3. Setup NER Pipeline\n",
    "We are using BERT-based NER model to identify and extract entities like names, organizations, and locations from the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf24922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NER pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 391/391 [00:00<00:00, 921.88it/s, Materializing param=classifier.weight]                                      \n",
      "BertForTokenClassification LOAD REPORT from: dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "Key                      | Status     |  | \n",
      "-------------------------+------------+--+-\n",
      "bert.pooler.dense.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NER pipeline loaded successfully!\n",
      "Model ready for entity extraction\n"
     ]
    }
   ],
   "source": [
    "# Initialize NER pipeline with BERT-based model\n",
    "print(\"Loading NER pipeline...\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "print(\" NER pipeline loaded successfully!\")\n",
    "print(\"Model ready for entity extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150d280",
   "metadata": {},
   "source": [
    "## 4. Extract Entities from Resume\n",
    "Here, we extract the important Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3b753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting named entities from resume...\n",
      "\n",
      "==================================================\n",
      "EXTRACTED ENTITIES\n",
      "==================================================\n",
      "\n",
      "PER:\n",
      "   • Sarah Johnson (confidence: 0.9940000176429749)\n",
      "\n",
      "ORG:\n",
      "   • Learning (confidence: 0.5519999861717224)\n",
      "   • ##Flow (confidence: 0.5989999771118164)\n",
      "   • PyTorch (confidence: 0.6650000214576721)\n",
      "   • AWS (confidence: 0.6990000009536743)\n",
      "   • Azure (confidence: 0.6790000200271606)\n",
      "   • ##QL (confidence: 0.37700000405311584)\n",
      "   • R (confidence: 0.4339999854564667)\n",
      "   • TensorFlow (confidence: 0.6650000214576721)\n",
      "   • PyTorch (confidence: 0.718999981880188)\n",
      "   • Scikit (confidence: 0.6660000085830688)\n",
      "   • Keras (confidence: 0.7319999933242798)\n",
      "   • React (confidence: 0.9509999752044678)\n",
      "   • Node (confidence: 0.8960000276565552)\n",
      "   • j (confidence: 0.492000013589859)\n",
      "   • F (confidence: 0.8669999837875366)\n",
      "   • ##k (confidence: 0.7390000224113464)\n",
      "   • Django (confidence: 0.4749999940395355)\n",
      "   • AWS (confidence: 0.8659999966621399)\n",
      "   • Azure (confidence: 0.824999988079071)\n",
      "   • Google Cloud (confidence: 0.7400000095367432)\n",
      "   • ##S (confidence: 0.3720000088214874)\n",
      "   • ##DB (confidence: 0.49799999594688416)\n",
      "   • TechCorp Inc (confidence: 0.9879999756813049)\n",
      "   • AWS (confidence: 0.9700000286102295)\n",
      "   • DataSolutions LLC (confidence: 0.9789999723434448)\n",
      "   • California (confidence: 0.9900000095367432)\n",
      "   • Berkeley (confidence: 0.8759999871253967)\n",
      "\n",
      "LOC:\n",
      "   • San Francisco (confidence: 0.9990000128746033)\n",
      "   • California (confidence: 0.9980000257492065)\n",
      "   • San Francisco (confidence: 0.996999979019165)\n",
      "   • California (confidence: 0.9990000128746033)\n",
      "\n",
      "MISC:\n",
      "   • AI (confidence: 0.984000027179718)\n",
      "   • Python (confidence: 0.968999981880188)\n",
      "   • Tensor (confidence: 0.652999997138977)\n",
      "   • Python (confidence: 0.7910000085830688)\n",
      "   • JavaScript (confidence: 0.8569999933242798)\n",
      "   • S (confidence: 0.9049999713897705)\n",
      "   • Platform (confidence: 0.7310000061988831)\n",
      "   • ##ial Intelligence (confidence: 0.7649999856948853)\n",
      "   • Natural Language Processing (confidence: 0.8759999871253967)\n",
      "   • Computer Vision (confidence: 0.7900000214576721)\n",
      "   • Neural Networks (confidence: 0.8230000138282776)\n",
      "   • Postgre (confidence: 0.824999988079071)\n",
      "   • ##QL (confidence: 0.8230000138282776)\n",
      "   • Mongo (confidence: 0.6940000057220459)\n",
      "   • MySQL (confidence: 0.921999990940094)\n",
      "   • Python (confidence: 0.9649999737739563)\n",
      "   • Stanford (confidence: 0.574999988079071)\n",
      "   • Google (confidence: 0.5329999923706055)\n",
      "\n",
      "Total entities found: 50\n",
      "Entity extraction completed!\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from resume text\n",
    "print(\"Extracting named entities from resume...\")\n",
    "entities = ner_pipeline(resume_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTRACTED ENTITIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group entities by type for better presentation\n",
    "entity_types = {}\n",
    "for entity in entities:\n",
    "    entity_type = entity['entity_group']\n",
    "    if entity_type not in entity_types:\n",
    "        entity_types[entity_type] = []\n",
    "    entity_types[entity_type].append({\n",
    "        'text': entity['word'], \n",
    "        'confidence': round(entity['score'], 3)\n",
    "    })\n",
    "\n",
    "# Display entities by type\n",
    "for entity_type, items in entity_types.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for item in items:\n",
    "        print(f\"   • {item['text']} (confidence: {item['confidence']})\")\n",
    "\n",
    "print(f\"\\nTotal entities found: {len(entities)}\")\n",
    "print(\"Entity extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32ce9a",
   "metadata": {},
   "source": [
    "## 5. Setup Text Summarization Pipeline\n",
    "We use DistilBART model for summarization, to create a concise resume summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc13853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading summarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please make sure the generation config includes `forced_bos_token_id=0`. \n",
      "Loading weights: 100%|██████████| 358/358 [00:00<00:00, 781.51it/s, Materializing param=model.shared.weight]                                  \n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization model loaded successfully!\n",
      "Model ready for text summarization\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading summarization model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "print(\"Summarization model loaded successfully!\")\n",
    "print(\"Model ready for text summarization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78c9d1",
   "metadata": {},
   "source": [
    "## 6. Generate Resume Summary\n",
    "Creating summary of the resume using the summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a38403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating resume summary...\n",
      "\n",
      "==================================================\n",
      " RESUME SUMMARY\n",
      "==================================================\n",
      " Sarah Johnson is an experienced Machine Learning Engineer with 5+ years of expertise in developing AI-powered solutions . She is fluent in Python programming, deep learning frameworks like TensorFlow and PyTorch, and cloud platforms like AWS and Azure . She has a track record in implementing natural language processing models and computer vision applications .\n",
      "==================================================\n",
      "\n",
      " Original text length: 1881 characters\n",
      " Summary length: 363 characters\n",
      " Summary generation completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate summary with appropriate length constraints\n",
    "print(\" Generating resume summary...\")\n",
    "\n",
    "# Tokenize and generate using the seq2seq model\n",
    "inputs = tokenizer(resume_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "summary_ids = summarization_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs.get(\"attention_mask\"),\n",
    "    max_length=150,\n",
    "    min_length=50,\n",
    "    do_sample=False,\n",
    "    early_stopping=True\n",
    ")\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" RESUME SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(summary_text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n Original text length: {len(resume_text)} characters\")\n",
    "print(f\" Summary length: {len(summary_text)} characters\")\n",
    "print(\" Summary generation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add469c",
   "metadata": {},
   "source": [
    "## 7. Setup Zero-Shot Classification Pipeline\n",
    "We are using BART-Large-MNLI to classify the resume into different skill domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51bb2091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading zero-shot classification pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 515/515 [00:00<00:00, 936.96it/s, Materializing param=model.shared.weight]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Zero-shot classification pipeline loaded successfully!\n",
      " Ready to classify into 5 skill domains\n",
      " Domains: Programming, Artificial Intelligence, Machine Learning, Web Development, Cybersecurity\n"
     ]
    }
   ],
   "source": [
    "# Initialize zero-shot classification pipeline\n",
    "print(\" Loading zero-shot classification pipeline...\")\n",
    "classification_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define skill domain labels for classification\n",
    "skill_domains = [\n",
    "    \"Programming\",\n",
    "    \"Artificial Intelligence\", \n",
    "    \"Machine Learning\",\n",
    "    \"Web Development\",\n",
    "    \"Cybersecurity\"\n",
    "]\n",
    "\n",
    "print(\" Zero-shot classification pipeline loaded successfully!\")\n",
    "print(f\" Ready to classify into {len(skill_domains)} skill domains\")\n",
    "print(\" Domains:\", \", \".join(skill_domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17439d17",
   "metadata": {},
   "source": [
    "## 8. Classify Resume into Skill Domains\n",
    "Here, we classify the resume content to determine which skill domains it best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39676861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classifying resume into skill domains...\n",
      "\n",
      "==================================================\n",
      " SKILL DOMAIN CLASSIFICATION\n",
      "==================================================\n",
      "1. Machine Learning: 46.7%\n",
      "   =========::::::::::: (46.7%)\n",
      "\n",
      "2. Artificial Intelligence: 28.6%\n",
      "   =====::::::::::::::: (28.6%)\n",
      "\n",
      "3. Programming: 11.7%\n",
      "   ==:::::::::::::::::: (11.7%)\n",
      "\n",
      "4. Cybersecurity: 6.8%\n",
      "   =::::::::::::::::::: (6.8%)\n",
      "\n",
      "5. Web Development: 6.2%\n",
      "   =::::::::::::::::::: (6.2%)\n",
      "\n",
      "==================================================\n",
      " Top domain: Machine Learning\n",
      " Confidence: 46.7%\n",
      " Classification completed!\n"
     ]
    }
   ],
   "source": [
    "# Perform zero-shot classification\n",
    "print(\" Classifying resume into skill domains...\")\n",
    "\n",
    "classification_result = classification_pipeline(resume_text, skill_domains)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" SKILL DOMAIN CLASSIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display results with confidence scores\n",
    "for i, (domain, score) in enumerate(zip(classification_result['labels'], classification_result['scores'])):\n",
    "    confidence_percent = round(score * 100, 1)\n",
    "    print(f\"{i+1}. {domain}: {confidence_percent}%\")\n",
    "    \n",
    "    # Add visual confidence bar\n",
    "    bar_length = int(confidence_percent / 5)  # Scale to 20 characters max\n",
    "    bar = \"=\" * bar_length + \":\" * (20 - bar_length)\n",
    "    print(f\"   {bar} ({confidence_percent}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\" Top domain: {classification_result['labels'][0]}\")\n",
    "print(f\" Confidence: {round(classification_result['scores'][0] * 100, 1)}%\")\n",
    "print(\" Classification completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5a251",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Results Summary\n",
    "Let's compile all our analysis results into a comprehensive resume parsing report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5df1091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " SMART RESUME PARSER - COMPREHENSIVE REPORT\n",
      "============================================================\n",
      "\n",
      " CANDIDATE INFORMATION\n",
      "----------------------------------------\n",
      " Name: Sarah Johnson\n",
      " Location: San Francisco, California\n",
      " Organizations: Learning, ##Flow, PyTorch\n",
      "\n",
      " RESUME SUMMARY\n",
      "----------------------------------------\n",
      " Sarah Johnson is an experienced Machine Learning Engineer with 5+ years of expertise in developing AI-powered solutions . She is fluent in Python programming, deep learning frameworks like TensorFlow and PyTorch, and cloud platforms like AWS and Azure . She has a track record in implementing natural language processing models and computer vision applications .\n",
      "\n",
      " SKILL DOMAIN CLASSIFICATION\n",
      "----------------------------------------\n",
      "1. Machine Learning: 46.7%\n",
      "2. Artificial Intelligence: 28.6%\n",
      "3. Programming: 11.7%\n",
      "\n",
      " ANALYSIS STATISTICS\n",
      "----------------------------------------\n",
      " Resume length: 1881 characters\n",
      " Entities extracted: 50\n",
      " Summary compression: 19.3%\n",
      " Top skill domain: Machine Learning\n",
      "\n",
      " MODELS USED\n",
      "----------------------------------------\n",
      "• NER: BERT-based model\n",
      "• Summarization: sshleifer/distilbart-cnn-12-6\n",
      "• Classification: facebook/bart-large-mnli\n",
      "\n",
      "============================================================\n",
      " RESUME PARSING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Resume Analysis Report\n",
    "print(\"=\"*60)\n",
    "print(\" SMART RESUME PARSER - COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n CANDIDATE INFORMATION\")\n",
    "print(\"-\" * 40)\n",
    "# Extract key entities for candidate info\n",
    "names = [entity['word'] for entity in entities if entity['entity_group'] == 'PER']\n",
    "locations = [entity['word'] for entity in entities if entity['entity_group'] == 'LOC'] \n",
    "organizations = [entity['word'] for entity in entities if entity['entity_group'] == 'ORG']\n",
    "\n",
    "print(f\" Name: {names[0] if names else 'Not detected'}\")\n",
    "print(f\" Location: {', '.join(locations[:2]) if locations else 'Not detected'}\")\n",
    "print(f\" Organizations: {', '.join(organizations[:3]) if organizations else 'Not detected'}\")\n",
    "\n",
    "print(\"\\n RESUME SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "print(summary_text)\n",
    "\n",
    "print(\"\\n SKILL DOMAIN CLASSIFICATION\")\n",
    "print(\"-\" * 40)\n",
    "for i, (domain, score) in enumerate(zip(classification_result['labels'][:3], classification_result['scores'][:3])):\n",
    "    confidence_percent = round(score * 100, 1)\n",
    "    print(f\"{i+1}. {domain}: {confidence_percent}%\")\n",
    "\n",
    "print(\"\\n ANALYSIS STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\" Resume length: {len(resume_text)} characters\")\n",
    "print(f\" Entities extracted: {len(entities)}\")\n",
    "print(f\" Summary compression: {round((len(summary_text)/len(resume_text))*100, 1)}%\")\n",
    "print(f\" Top skill domain: {classification_result['labels'][0]}\")\n",
    "\n",
    "print(\"\\n MODELS USED\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• NER: BERT-based model\")\n",
    "print(\"• Summarization: sshleifer/distilbart-cnn-12-6\") \n",
    "print(\"• Classification: facebook/bart-large-mnli\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" RESUME PARSING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
